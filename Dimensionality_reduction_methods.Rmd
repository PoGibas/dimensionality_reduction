Dimensionality Reduction Methods
========================================================
author: Povilas Gibas
date: `r Sys.Date()`
autosize: true

Content
========================================================

- Bullet 1
- Bullet 2
- Bullet 3

High-Dimensionality Problem
========================================================

Hairy dog/ball prob problem

DRMs
========================================================

Introduce notation

$X$ - matrix of m n dimension m - variables and n - samples

Classification of DRMs
========================================================

Goal
Algorithm
Extensions
Issues
When works/doesn't work
Implementation
  Algorithm
  R

PCA
========================================================
<!--
  Variance shows how much variables change together:
    sigma = E ((xi - xmean)^2 / (n - 1))
  The rows of P are the principal components of X
  PCA tries to de-correlate the original data by finding directions in which variance in maximized (or minimized for off-diagonal)
  V %*% t(V) makes an identity matrix, this V %*% t(V) %*% lambda %*% V %*% t(V) just returns lambda (eigenvalues) 
-->   

- Goal    
  Find orthonormal transformation matrix $P$ with which $Y = PX$ such that $C_Y = \frac{1}{n - 1} YY^{T}$ is diagonalized.   

  $C_X = \frac{1}{n - 1}XX^{T}$ shows how much variables change together and we want $C_Y$ where:
    1. Maximum variance on the diagonal (max difference between samples);
    2. Minimum off-diagonal variance (non-zero values are caused by noise and redundancy).   
    $ $

  $C_Y = \frac{1}{n - 1}YY^{T} = \frac{1}{n - 1} PX(PX)^{T} = \frac{1}{n - 1} PXX^{T}P^{T} = \frac{1}{n - 1} = PAP^{T} = \frac{1}{n - 1} V^{T}(V \Lambda V^{T})V^{T} = \frac{1}{n - 1} \Lambda$

  $P$ - m-by-m matrix of weights whose columns are the eigenvectors of XTX (linear projection for a high dimensional space)
  $V$ - eigenvector
  $\Lambda$ - diagonal matrix of eigenvalues

- Algorithm
  0.
    0.1. Scale - equalizes variance (divide variable by it's variance);  
    0.2. Center - equalizes mean (divided by XXX);
  1. Get similarity matrix $C_X$ (covariance, correlation, etc).  
  2. Calculate eigenvalues and eigenvectors.  
  3. ?? Use principal components

- Extensions
- Issues
  - Different scale - when one variable dominates the variance simply because it's in a higher scale
- When works/doesn't work
- Implementation
  - Computationally scales O(np2 + p3)
  - stats::prcomp

- Cite Pearson 1901
- difference from svd?

kPCA
========================================================

Classical scalling
========================================================

Isomap
========================================================

LLE
========================================================

Qualities of DR methods
========================================================

Applications
========================================================

- R packages
- Solving problems
  - Swiss roll
  - nipt
