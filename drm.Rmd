Dimensionality Reduction Methods
========================================================
author: Povilas Gibas
date: `r Sys.Date()`
autosize: true

Content
========================================================

- Bullet 1
- Bullet 2
- Bullet 3

High-Dimensionality Problem
========================================================

Hairy dog/ball prob problem

DRMs
========================================================

Introduce notation

$X$ - matrix of m n dimension m - variables and n - samples

Classification of DRMs
========================================================

Goal
Algorithm
Extensions
Issues
When works/doesn't work
Implementation
  Algorithm
  R

PCA
========================================================
<!--
  Variance shows how much variables change together:
    sigma = E ((xi - xmean)^2 / (n - 1))
  The rows of P are the principal components of X
  PCA tries to de-correlate the original data by finding directions in which variance in maximized (or minimized for off-diagonal)
  V %*% t(V) makes an identity matrix, this V %*% t(V) %*% lambda %*% V %*% t(V) just returns lambda (eigenvalues) 
-->   

- Goal    
  Find orthonormal transformation matrix $P$ with which $Y = PX$ such that $C_Y = \frac{1}{n - 1} YY^{T}$ is diagonalized.   

  $C_X = \frac{1}{n - 1}XX^{T}$ shows how much variables change together and we want $C_Y$ where:
    1. Maximum variance on the diagonal (max difference between samples);
    2. Minimum off-diagonal variance (non-zero values are caused by noise and redundancy).   
    $ $

  $C_Y = \frac{1}{n - 1}YY^{T} = \frac{1}{n - 1} PX(PX)^{T} = \frac{1}{n - 1} PXX^{T}P^{T} = \frac{1}{n - 1} = PAP^{T} = \frac{1}{n - 1} V^{T}(V \Lambda V^{T})V^{T} = \frac{1}{n - 1} \Lambda$

  $P$ - m-by-m matrix of weights whose columns are the eigenvectors of XTX (linear projection for a high dimensional space)
  $V$ - eigenvector
  $\Lambda$ - diagonal matrix of eigenvalues

- Algorithm
  0.
    0.1. Scale - equalizes variance (divide variable by it's variance);  
    0.2. Center - equalizes mean (divided by XXX);
  1. Get similarity matrix $C_X$ (covariance, correlation, etc).  
  2. Calculate eigenvalues and eigenvectors.  
  3. ?? Use principal components

- Extensions
- Issues
  - Requires that variables are linearly related   
  - Different scale - when one variable dominates the variance simply because it's in a higher scale
- When works/doesn't work
- Implementation
  - Computationally scales O(np2 + p3)
  - stats::prcomp

- Cite Pearson 1901
- difference from svd?


kPCA
========================================================

# kPCA (https://www.youtube.com/watch?v=HbDHohXPLnU&frags=pl%2Cwn)
  kernalized PCA
  non-linear DR 

Linear subspaces may not adequative represent underlying madifold which might have non-linear structure.
Example:
  swiss roll
  tanenbaum faces
  bulls eye

  2D x1, x2
  3D (we add one dimension): x1, x2, x1^2 + x2^3
  ie omega: R2 -> R2

datasets that are not lineary seperanle can be made into lineary seperanle

kernel trick:
  kernel matrix contains all pairwise evaluations of dot products
  scales with d

algo
  pick a kernel function k(xi, xj)
  calcialte the kerlenl matrix
  center the kernel matrix
  solve the eigenproblem Kalphai
  Project the data to each new dimension j

other non-linear dr
laplacian eigenmaps
LLE
ospmap
MDS
feedforward autoencoders

Classical scalling
========================================================

Isomap
========================================================

LLE
========================================================

Qualities of DR methods
========================================================

Applications
========================================================

- R packages
- Solving problems
  - Swiss roll
  - nipt
